---
title: "Evaluate Recommender: Module Reference"
titleSuffix: Azure Machine Learning service
description: Learn how to use the Evaluate Recommender module in Azure Machine Learning service to evaluate the accuracy of recommender model predictions.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference

author: likebupt
ms.author: keli19
ms.date: 10/10/2019
---
# Evaluate Recommender

This article describes how to use the **Evaluate Recommender** module in Azure Machine Learning designer (preview), to measure the accuracy of predictions made by a recommendation model. Using this module, you can evaluate four different kinds of recommendations:  
  
-   Ratings predicted for a given user and item  
  
-   Items recommended for a given user  
  
When you create predictions using a recommendation model, slightly different results are returned for each of these supported prediction types. The **Evaluate Recommender** module deduces the kind of prediction from the column format of the scored dataset. For example, the **scored dataset** might contain:

- user-item-rating triples
- users and their recommended items

The module also applies the appropriate performance metrics, based on the type of prediction being made. 

  
## How to configure Evaluate Recommender

The **Evaluate Recommender** module compares the predictions output by a recommendation model with the corresponding "ground truth" data. For example, the [Score SVD Recommender](score-svd-recommender.md) module produces scored datasets that can be analyzed with **Evaluate Recommender**.

### Requirements

**Evaluate Recommender** requires the following datasets as input. 
  
#### Test dataset

The **test dataset** contains the "ground truth" data in the form of **user-item-rating triples**.  

#### Scored dataset

The **scored dataset** contains the predictions that were generated by the recommendation model.  
  
The columns in this second dataset depend on the kind of prediction you were performing during scoring. For example, the scored dataset might contain any of the following:

- Users, items, and the ratings the user would likely give for the item
- A list of users and items recommended for them 

### Metrics

Performance metrics for the model are generated based on the type of input. For details, see these sections:

+ [Evaluate predicted ratings](#evaluate-predicted-ratings)
+ [Evaluate item recommendations](#evaluate-item-recommendations)

## Evaluate predicted ratings  

When evaluating predicted ratings, the scored dataset (the second input to **Evaluate Recommender**) must contain **user-item-rating triples**, meeting these requirements:
  
-   The first column of the dataset contains user identifiers.  
  
-   The second column contains the item identifiers.  
  
-   The third column contains the corresponding user-item ratings.  
  
> [!IMPORTANT] 
> For evaluation to succeed, the column names must be `User`, `Item`, and `Rating`, respectively.  
  
**Evaluate Recommender** compares the ratings in the ground truth dataset to the predicted ratings of the scored dataset, and computes the **mean absolute error** (MAE) and the **root mean squared error** (RMSE).



## Evaluate item recommendations

When evaluating item recommendation, use a scored dataset that includes the recommended items for each user:
  
-   The first column of the dataset must contain the user identifier.  
  
-   All subsequent columns should contain the corresponding recommended item identifiers, ordered by how relevant an item is to the user. 

    Before connecting this dataset, we recommend that you sort the dataset so that the most relevant items come first.  



> [!IMPORTANT] 
> For **Evaluate Recommender** to work, the column names must be `User`, `Item 1`, `Item 2`, `Item 3` and so forth.  
  
**Evaluate Recommender** computes the average normalized discounted cumulative gain (**NDCG**) and returns it in the output dataset.  
  
Because it is impossible to know the actual "ground truth" for the recommended items, **Evaluate Recommender** uses the user-item ratings in the test dataset as gains in the computation of the NDCG. To evaluate, the recommender scoring module must only produce recommendations for items with ground truth ratings (in the test dataset).  
  

## Next steps

See the [set of modules available](module-reference.md) to Azure Machine Learning service. 
