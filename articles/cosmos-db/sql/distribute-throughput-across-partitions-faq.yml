### YamlMime:FAQ
metadata:
  title: Frequently asked questions on distributing throughput across partitions in Azure Cosmos DB (preview)
  description: Frequently asked questions about distributing throughput across partitions in Azure Cosmos DB
  author: seesharprun
  ms.author: sidandrews
  ms.service: cosmos-db
  ms.custom: event-tier1-build-2022
  ms.topic: faq
  ms.reviewer: dech
  ms.date: 05/09/2022
title: Frequently asked questions on distributing throughput across partitions in Azure Cosmos DB (preview)
summary: |
  [!INCLUDE[appliesto-sql-mongodb-api](../includes/appliesto-sql-mongodb-api.md)]
  
  The throughput redistribution feature of Azure Cosmos DB gives you the ability to redistribute your provisioned throughput across physical partitions. This article answers commonly asked questions about Azure Cosmos DB throughput redistribution across partitions.

sections:
  - name: General
    questions:
      - question: |
          What resources can I use this feature on?
        answer: |
          The feature is only supported for SQL and API for MongoDB accounts and on collections with dedicated throughput (either manual or autoscale). Shared throughput databases aren't supported in the preview.
      - question: |
          Which version of the Azure Cosmos DB functionality in Azure PowerShell and Azure CLI supports this feature?
        answer: |
          The ability to redistribute RU/s across physical partitions is only supported in the latest preview version of Azure PowerShell and Azure CLI.
      - question: |
          What is the maximum number of physical partitions I can change in one request?
        answer: |
          - The maximum number of source and physical partitions that can be included in a single request is 20 each.
          - You must provide at least one source and one target physical partition in each request. The source partition(s) must have enough RU/s to redistribute to the target partition(s).
          - The desired RU/s for each target physical partition can't exceed 10,000 RU/s or the total RU/s of the overall resource. If your desired RU/s is greater than the RU/s of the overall resource, increase your overall RU/s first before redistributing the RU/s.
      - question: |
          Is there a limit on how frequently I can make a call to redistribute throughput across partitions?
        answer: |
          You can make a maximum of five requests per minute to redistribute throughput across partitions. 
      - question: |
          What happens to my RU/s distribution when I change the overall RU/s?
        answer: |
          - If you lower your RU/s, each physical partition gets the equivalent fraction of the new RU/s (`current throughput fraction * new RU/s`). For example, suppose you have a collection with 6000 RU/s and 3 physical partitions. You scale it down to 3000 RU/s.

              |Before scale-down (6000 RU/s)  |After scale down (3000 RU/s)  |Fraction of total RU/s  |
              |---------|---------|---------|
              |P0: 1000 RU/s     | P0: 500 RU/s        |  1/6       |
              |P1: 4000 RU/s      |  P1: 1000 RU/s       |   2/3      |
              |P2: 1000 RU/s      |  P2: 500 RU/s       |       1/6  |

          - If you increase your RU/s without triggering a split - that is, you scale to a total RU/s <= current partition count * 10,000 RU/s - each physical partition will have RU/s = `MIN(current throughput fraction * new RU/s, 10,000 RU/s)`. Consider an example where the resulting sum of all RU/s across all partitions is less than the total new RU/s of the resource. It's recommended to reset your RU/s to an even distribution and redistribute to ensure that all available RU/s are allocated to a partition. To check if this scenario applies to your resource use Azure Monitor metrics. Compare the value of the **ProvisionedThroughput** (when using manual throughput) or **AutoscaleMaxThroughput** (when using autoscale) metric to the value of the **PhysicalPartitionThroughput** metric. If the value of **PhysicalPartitionThroughput** is less than the respective **ProvisionedThroughput** or **AutoscaleMaxThroughput**, then reset your RU/s to an even distribution before redistributing, or lower your resource's throughput to the value of **PhysicalPartitionThroughput**. 

              For example, suppose you have a collection with 6000 RU/s and 3 physical partitions. You scale it up to 12,000 RU/s:

              |Before scale-up (6000 RU/s)  |After scale up (12,000 RU/s)  |Fraction of total RU/s  |
              |---------|---------|---------|
              |P0: 1000 RU/s     | P0: 2000 RU/s        |  1/6       |
              |P1: 4000 RU/s      |  P1: 8000 RU/s       |   2/3      |
              |P2: 1000 RU/s      |  P2: 2000 RU/s       |       1/6  |

          - If you increase your RU/s [beyond what the current partition layout can serve](../scaling-provisioned-throughput-best-practices.md), you trigger a split. By design, all physical partitions will default to having the same number of RU/s. After partitions split, the logical partitions that contributed to a hot partition may be on a different physical partition. If necessary, you can redistribute your RU/s on the new layout.
